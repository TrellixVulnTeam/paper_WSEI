{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T07:36:19.592293Z",
     "start_time": "2019-04-16T07:36:17.240064Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "sys.path.append('/data/CaoZhong/utils/')\n",
    "from my_utils import *\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T07:36:20.190317Z",
     "start_time": "2019-04-16T07:36:19.595471Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ctx = mx.gpu(4)\n",
    "train_batch_size = 32\n",
    "test_batch_size = 512\n",
    "model_name = 'attention_gluon'\n",
    "data_path = '../data/dataset_sub_gluon.pkl'\n",
    "data_iter = DataIter(data_path, train_batch_size, test_batch_size)\n",
    "user_count, item_count, cate_count = data_iter.get_count()\n",
    "train_iter, test_iter = data_iter.get_data_iter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T07:36:20.407260Z",
     "start_time": "2019-04-16T07:36:20.192873Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uid shape:  (32,) cpu(0)\n",
      "hist shape:  (32, 364) cpu(0)\n",
      "hist_cate shape:  (32, 364) cpu(0)\n",
      "pre shape:  (32,) cpu(0)\n",
      "cate shape:  (32,) cpu(0)\n",
      "label shape:  (32,) cpu(0)\n",
      "sl shape:  (32,) cpu(0)\n",
      "uid shape:  (512,)\n",
      "hist shape:  (512, 430)\n",
      "hist_cate shape:  (512, 430)\n",
      "pre shape:  (512,)\n",
      "cate shape:  (512,)\n",
      "label shape:  (512,)\n",
      "sl shape:  (512,)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_iter:\n",
    "    for name, data in zip(['uid','hist','hist_cate','pre','cate','label','sl'], batch):\n",
    "        print(name, 'shape: ', data.shape, data.context)        \n",
    "    break\n",
    "for batch in test_iter:\n",
    "    for name, data in zip(['uid','hist','hist_cate','pre','cate','label','sl'], batch):\n",
    "        print(name, 'shape: ', data.shape)        \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意力机制（1）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T13:14:31.106195Z",
     "start_time": "2019-04-16T13:14:31.093000Z"
    }
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Block):\n",
    "    def __init__(self, attention_size, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        self.model = nn.Sequential()\n",
    "        self.model.add(nn.Dense(attention_size, activation='tanh', use_bias=False, flatten=False))\n",
    "        self.model.add(nn.Dense(1, use_bias=False, flatten=False))\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        query:    [B, H]\n",
    "        keys:     [T, B, H]\n",
    "        sl:       [B]\n",
    "    \"\"\"\n",
    "    def forward(self, keys, query,sl):\n",
    "        query = nd.broadcast_axis(query.expand_dims(0), axis=0, size=keys.shape[0])\n",
    "        item_history = nd.concat(keys, query, dim=2)\n",
    "        e = self.model(item_history)   # [T, B, 1]\n",
    "        e = nd.SequenceMask(e, sl.reshape((-1)), use_sequence_length=True, value=(-2 ** 32 + 1))\n",
    "        alpha = nd.softmax(e, axis=0)        # [T, B, 1]\n",
    "\n",
    "        return (alpha * keys).sum(axis=0)  # [T, B, 1] * [T, B, H]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T13:10:23.775469Z",
     "start_time": "2019-04-16T13:10:23.766319Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class HybridNet(nn.HybridBlock):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(HybridNet, self).__init__(**kwargs)\n",
    "        self.hidden = nn.Dense(10)\n",
    "        self.output = nn.Dense(2)\n",
    "\n",
    "    def hybrid_forward(self, F, x,y):\n",
    "        x = x.expand_dims(0)\n",
    "        y = y.expand_dims(0)\n",
    "        x = F.broadcast_axis(x, axis=0, size=len(x))\n",
    "        y = F.broadcast_axis(y, axis=0, size=len(y))\n",
    "#         c = nd.concat(x,y,dim=0)\n",
    "        return self.output(x*y)\n",
    "#         x = F.relu(self.hidden(x))\n",
    "#         y = F.relu(self.hidden(y))\n",
    "#         z = x + y\n",
    "#         z = self.output(z)\n",
    "#         print(z)\n",
    "#         return z.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T13:13:54.758861Z",
     "start_time": "2019-04-16T13:13:54.720130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.06148639  0.0229429 ]]\n",
       "<NDArray 1x2 @cpu(0)>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = HybridNet()\n",
    "net.initialize()\n",
    "# net.hybridize()\n",
    "x = nd.random.normal(shape=(1, 4))\n",
    "c = nd.random.normal(shape=(1,2))\n",
    "net(x,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T13:16:01.731277Z",
     "start_time": "2019-04-16T13:16:01.711510Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.79998255  0.53840852  0.4154864   0.31237507]]\n",
      "<NDArray 1x4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "#  检测注意力机制\n",
    "T, B, H = 3, 1, 4\n",
    "att_model = Attention(5)\n",
    "att_model.initialize()\n",
    "sl = nd.array([2])\n",
    "history = nd.random_uniform(shape=(T, B, H))\n",
    "item = nd.ones(shape=(B, H))\n",
    "c = model_t(history, item,sl)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nd.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T07:36:21.631355Z",
     "start_time": "2019-04-16T07:36:21.627542Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# class Model(nn.Block):\n",
    "#     def __init__(self, item_count, cate_count, embed_size, attention_size, num_hiddens,  ctx, **kwargs):\n",
    "#         super(Model, self).__init__(**kwargs)\n",
    "#         self.num_hiddens = num_hiddens\n",
    "#         self.item_embedding = nn.Embedding(item_count, embed_size)\n",
    "#         self.cate_embedding = nn.Embedding(cate_count, embed_size)\n",
    "#         self.batch_normal_layer = nn.BatchNorm()\n",
    "#         self.dense_layer = nn.Dense(num_hiddens)\n",
    "        \n",
    "#         self.attention = attention_model(attention_size)\n",
    "        \n",
    "#         self.mlp = nn.Sequential()\n",
    "#         self.mlp.add(nn.BatchNorm())\n",
    "#         self.mlp.add(nn.Dense(80, activation='sigmoid'))\n",
    "#         self.mlp.add(nn.Dense(40, activation='sigmoid'))\n",
    "#         self.mlp.add(nn.Dense(1, activation=None))\n",
    "    \n",
    "#     def forward(self, item, cate, hist, hist_cate, ls,):\n",
    "        \n",
    "#         item = item.reshape((-1))                         # [B]\n",
    "#         item_emb_w = self.item_embedding(item)            # [B, E]\n",
    "        \n",
    "#         cate_emb_w = self.cate_embedding(cate)\n",
    "#         i_emb = nd.concat(item_emb_w, cate_emb_w,dim=1)   # [B, 2E]\n",
    "        \n",
    "#         hi_emb = self.item_embedding(hist)                 # [B, T, E]\n",
    "#         hc_emb = self.cate_embedding(hist_cate)                 # [B, T, E]\n",
    "#         h_emb = nd.concat(hi_emb, hc_emb, dim=-1)          # [B, T, 2E]\n",
    "          \n",
    "#         user_emb = attention_forward(self.attention, h_emb.swapaxes(0,1), i_emb, ls)  # [B, 2E]\n",
    "#         user_emb = self.batch_normal_layer(user_emb)\n",
    "        \n",
    "#         user_emb = self.dense_layer(user_emb)\n",
    "#         din = nd.concat(user_emb, i_emb, dim=-1)\n",
    "#         score = self.mlp(din)\n",
    "#         return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 混合模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T07:37:16.688048Z",
     "start_time": "2019-04-16T07:37:16.666541Z"
    }
   },
   "outputs": [],
   "source": [
    "class HybridModel(nn.HybridBlock):\n",
    "    def __init__(self, item_count, cate_count, embed_size, attention_size, num_hiddens,  ctx, **kwargs):\n",
    "        super(HybridModel, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.item_embedding = nn.Embedding(item_count, embed_size)\n",
    "        self.cate_embedding = nn.Embedding(cate_count, embed_size)\n",
    "        self.batch_normal_layer = nn.BatchNorm()\n",
    "        self.dense_layer = nn.Dense(num_hiddens)\n",
    "        \n",
    "        self.attention = attention_model(attention_size)\n",
    "        \n",
    "        self.mlp = nn.HybridSequential()\n",
    "        self.mlp.add(nn.BatchNorm())\n",
    "        self.mlp.add(nn.Dense(80, activation='sigmoid'))\n",
    "        self.mlp.add(nn.Dense(40, activation='sigmoid'))\n",
    "        self.mlp.add(nn.Dense(1, activation=None))\n",
    "    \n",
    "    def forward(self,F, item, cate, hist, hist_cate, ls,time_length):\n",
    "        \n",
    "        item = item.reshape((-1))                         # [B]\n",
    "        item_emb_w = self.item_embedding(item)            # [B, E]\n",
    "        \n",
    "        cate_emb_w = self.cate_embedding(cate)\n",
    "        i_emb = F.concat(item_emb_w, cate_emb_w,dim=1)   # [B, 2E]\n",
    "        \n",
    "        hi_emb = self.item_embedding(hist)                 # [B, T, E]\n",
    "        hc_emb = self.cate_embedding(hist_cate)                 # [B, T, E]\n",
    "        h_emb = F.concat(hi_emb, hc_emb, dim=-1)          # [B, T, 2E]\n",
    "        \n",
    "  \n",
    "        user_emb = attention_forward(F, self.attention, h_emb.swapaxes(0,1), i_emb, ls, time_length)  # [B, 2E]\n",
    "        user_emb = self.batch_normal_layer(user_emb)\n",
    "        \n",
    "        user_emb = self.dense_layer(user_emb)\n",
    "        din = F.concat(user_emb, i_emb, dim=-1)\n",
    "        score = self.mlp(din)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T07:37:16.943789Z",
     "start_time": "2019-04-16T07:37:16.938517Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# T, B= 5, 2\n",
    "# item_count, cate_count, embed_size, attention_size,num_hiddens = 4, 3, 3, 3, 6\n",
    "# cate_list = nd.array([0, 1, 3, 0], ctx=ctx)\n",
    "# history = nd.array([[1, 1, 2,3,0],[2,2,2,0,0]], ctx=ctx)\n",
    "# hist_cate = cate_list[history]\n",
    "# item = nd.array([3,4], ctx=ctx)\n",
    "# cate = cate_list[item]\n",
    "# sl = nd.array([4,3], ctx=ctx)\n",
    "# net = Model(item_count, cate_count, embed_size,attention_size, num_hiddens, ctx)\n",
    "# net.initialize(init=init.Xavier(),force_reinit=True, ctx=ctx)\n",
    "# net(item, cate, history, hist_cate, sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T07:37:17.209707Z",
     "start_time": "2019-04-16T07:37:17.194000Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, lr, num_epochs, ctx):\n",
    "    auc_list, loss_list, x_vals= [], [], []\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate':lr})\n",
    "    \n",
    "    global_step = 1\n",
    "    stime = time.time()\n",
    "    stime2 = time.time()\n",
    "    print('auc: %.4f' % (eval_auc(net,test_iter, ctx)))\n",
    "    epoch_bar = tqdm_notebook(range(1, num_epochs+1))\n",
    "    for epoch in epoch_bar:\n",
    "        l_sum = 0.0\n",
    "        bar = tqdm_notebook(train_iter)\n",
    "        for batch in bar:\n",
    "            \n",
    "            uid, hist, hist_cate, item, cate, label, sl = [data.as_in_context(ctx) for data in batch[:-1]]\n",
    "            time_length = batch[-1]\n",
    "            with autograd.record():\n",
    "                pred = net(item,cate,hist,hist_cate, sl,time_length)\n",
    "                l = loss(pred, label)\n",
    "            l.backward()\n",
    "            trainer.step(train_batch_size)\n",
    "            \n",
    "            l_sum += l.mean().asscalar()\n",
    "            \n",
    "            if global_step % 1000 ==0:\n",
    "                test_auc = eval_auc(net,test_iter, ctx)\n",
    "                tip = \"epoch %d, global step:%d, loss %.4f, test auc:%.4f, time:%.2f\" % (epoch,global_step,  l_sum/1000, test_auc, time.time()-stime2)\n",
    "                tip_info(tip, out=False)\n",
    "                bar.set_description_str(tip)\n",
    "                loss_list.append(l_sum/1000)\n",
    "                auc_list.append(test_auc)\n",
    "                x_vals.append(global_step//1000)\n",
    "                l_sum = 0.0\n",
    "                stime2 = time.time()\n",
    "            global_step += 1\n",
    "        tip = 'epoch %d done, cost time:%.2f' % (epoch, time.time() - stime)\n",
    "        tip_info(tip, out=False)\n",
    "        epoch_bar.set_description_str(tip)\n",
    "    return loss_list, auc_list, x_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T07:37:17.508929Z",
     "start_time": "2019-04-16T07:37:17.472095Z"
    }
   },
   "outputs": [],
   "source": [
    "loss = gloss.SigmoidBinaryCrossEntropyLoss()\n",
    "net = HybridModel(item_count, cate_count, 64, 10, 128, ctx)\n",
    "net.initialize(init=init.Xavier(),force_reinit=True, ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T07:43:22.847537Z",
     "start_time": "2019-04-16T07:37:17.988801Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc: 0.4981\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a40da1181c144aba83a914c8498da03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910c066bf92448e9b5a047562206b56e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2151), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2d686746244c91ab37388745ce6588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2151), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-00bc515b0970>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_vals\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-7f3db8378488>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_iter, test_iter, lr, num_epochs, ctx)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0ml_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1892\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The current array is not a scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1894\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1896\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1874\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1875\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1876\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   1877\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_list, auc_list, x_vals= train(net,train_iter, test_iter, 0.1, 30, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T15:59:32.338176Z",
     "start_time": "2019-04-15T15:59:32.327392Z"
    }
   },
   "outputs": [],
   "source": [
    "file_name = 'train_result_'+model_name+'.pkl'\n",
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump(loss_list, f,pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(auc_list, f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
